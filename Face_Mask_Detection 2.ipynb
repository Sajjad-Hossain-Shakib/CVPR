{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import os\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 100)     2800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 100)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 100)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                6480050   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 6,573,052\n",
      "Trainable params: 6,573,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model for image classification\n",
    "model =Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.5),        # prevent overfitting by reducing the model's reliance on any one feature\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2, activation='softmax') # Used in multiclass classification problems to convert the output of the model into a probability distribution over the classes.\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model Optimizer, Loss and Accuracy Metrics\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', # minimize this difference\n",
    "              metrics=['acc']) # measures the percentage of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3268 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Setting up an image data generator for training the model\n",
    "TRAINING_DIR = \"D:/CVPR/CVPR2/Dataset/train\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, #number of samples in each batch\n",
    "                                                    target_size=(150, 150)) #resizes the images to 150x150 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 818 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Setting up an image data generator for validation data to evaluate the performance of the model.\n",
    "VALIDATION_DIR = \"D:/CVPR/CVPR2/Dataset/test\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255) #image transformation,Normalizing by rescaling the pixel values to be between 0 and 1\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filepath+\"train_datagen\", 'rb')\n",
    "train_datagen = pickle.load(file)\n",
    "file.close()\n",
    "import pickle\n",
    "\n",
    "filepath = \"Pickle Data/\"\n",
    "file = open(filepath+\"validation_datagen\", 'rb')\n",
    "train_datagen = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_19644\\680011608.py:4: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/10\n",
      " 27/327 [=>............................] - ETA: 3:34 - loss: 0.9228 - acc: 0.6630"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\cvpr\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327/327 [==============================] - ETA: 0s - loss: 0.5115 - acc: 0.7840WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\envs\\cvpr\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From c:\\Users\\Asus\\anaconda3\\envs\\cvpr\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model2-001.model\\assets\n",
      "327/327 [==============================] - 257s 787ms/step - loss: 0.5115 - acc: 0.7840 - val_loss: 0.4383 - val_acc: 0.8631\n",
      "Epoch 2/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.4167 - acc: 0.8375INFO:tensorflow:Assets written to: model2-002.model\\assets\n",
      "327/327 [==============================] - 270s 826ms/step - loss: 0.4167 - acc: 0.8375 - val_loss: 0.2788 - val_acc: 0.9120\n",
      "Epoch 3/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.3739 - acc: 0.8433INFO:tensorflow:Assets written to: model2-003.model\\assets\n",
      "327/327 [==============================] - 314s 960ms/step - loss: 0.3739 - acc: 0.8433 - val_loss: 0.2372 - val_acc: 0.9108\n",
      "Epoch 4/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.3640 - acc: 0.8537INFO:tensorflow:Assets written to: model2-004.model\\assets\n",
      "327/327 [==============================] - 316s 965ms/step - loss: 0.3640 - acc: 0.8537 - val_loss: 0.2077 - val_acc: 0.9205\n",
      "Epoch 5/10\n",
      "327/327 [==============================] - 298s 911ms/step - loss: 0.3475 - acc: 0.8583 - val_loss: 0.2392 - val_acc: 0.8985\n",
      "Epoch 6/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.3223 - acc: 0.8700INFO:tensorflow:Assets written to: model2-006.model\\assets\n",
      "327/327 [==============================] - 295s 901ms/step - loss: 0.3223 - acc: 0.8700 - val_loss: 0.1951 - val_acc: 0.9352\n",
      "Epoch 7/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.2974 - acc: 0.8810INFO:tensorflow:Assets written to: model2-007.model\\assets\n",
      "327/327 [==============================] - 284s 868ms/step - loss: 0.2974 - acc: 0.8810 - val_loss: 0.1790 - val_acc: 0.9413\n",
      "Epoch 8/10\n",
      "327/327 [==============================] - 281s 859ms/step - loss: 0.2950 - acc: 0.8871 - val_loss: 0.1823 - val_acc: 0.9340\n",
      "Epoch 9/10\n",
      "327/327 [==============================] - 276s 843ms/step - loss: 0.3063 - acc: 0.8782 - val_loss: 0.1874 - val_acc: 0.9377\n",
      "Epoch 10/10\n",
      "327/327 [==============================] - ETA: 0s - loss: 0.2994 - acc: 0.8791INFO:tensorflow:Assets written to: model2-010.model\\assets\n",
      "327/327 [==============================] - 286s 873ms/step - loss: 0.2994 - acc: 0.8791 - val_loss: 0.1734 - val_acc: 0.9328\n"
     ]
    }
   ],
   "source": [
    "#Model Fitting\n",
    "checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=10,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint]) # After each epoch,the ModelCheckpoint callback will save the model if its validation loss is lower than the previous best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save('Face_mask_detection_model_2.2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking live input through camera to detect face mask\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Path to the model file\n",
    "model_path = \"./Face_mask_detection_model_2.2.h5\"\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "labels_dict = {0: 'No Mask', 1: 'Mask on'}\n",
    "color_dict = {0: (0, 0, 255), 1: (0, 255, 0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0)  # Use camera 0\n",
    "\n",
    "# Load the pre-trained classifier used by OpenCV to detect faces.\n",
    "classifier = cv2.CascadeClassifier(\"D:/CVPR/CVPR2/opencv-master/data/haarcascades/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im = cv2.flip(im, 1, 1)  # Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # Detect faces\n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f]  # Scale the shape\n",
    "        # Adjust the size of the detection box\n",
    "        detection_box_size = int(max(w, h) * 1.2)\n",
    "        x -= (detection_box_size - w) // 2\n",
    "        y -= (detection_box_size - h) // 2\n",
    "        w = h = detection_box_size\n",
    "        \n",
    "        # Make sure the box doesn't go outside the image boundaries\n",
    "        x = max(0, x)\n",
    "        y = max(0, y)\n",
    "        w = min(im.shape[1] - x, w)\n",
    "        h = min(im.shape[0] - y, h)\n",
    "        \n",
    "        face_img = im[y:y + h, x:x + w]\n",
    "        resized = cv2.resize(face_img, (150, 150))\n",
    "        normalized = resized / 255.0\n",
    "        reshaped = np.reshape(normalized, (1, 150, 150, 3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result = model.predict(reshaped)\n",
    "\n",
    "        label = np.argmax(result, axis=1)[0]\n",
    "\n",
    "        cv2.rectangle(im, (x, y), (x + w, y + h), color_dict[label], 2)\n",
    "        cv2.rectangle(im, (x, y - 40), (x + w, y), color_dict[label], -1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Mask_Detection', im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if q key is pressed, break out of the loop\n",
    "    if key == 113:  # The q key\n",
    "        break\n",
    "\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr tf2.4 py3.8",
   "language": "python",
   "name": "cvpr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
